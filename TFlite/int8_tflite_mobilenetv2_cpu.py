# -*- coding: utf-8 -*-
"""int8_tflite_mobilenetv2_cpu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V1Gt6MpJDh79fOj22ENqwuXfj7TScLuA
"""

import os
import time
import datetime
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm
from sklearn.metrics import accuracy_score, classification_report
import tensorflow as tf
import psutil
import gc

# === CONFIG ===
IMG_SIZE = 224
IMG_DIR = '/content/drive/MyDrive/test'  # Folder with subfolders per class
MODEL_PATH = '/content/drive/MyDrive/models/fp16_tflite/mobilenetv2_int8.tflite'  # FP16 TFLite model
ARCHITECTURE = "mobilenetv2"
FORMAT_TYPE = "TFLite_int8"
RUNTIME = "TFLite"
NUM_REPEATS = 100
DEVICE = "CPU"

# === OUTPUT PATHS ===
# Save reports in your Drive
REPORT_DIR = "/content/drive/MyDrive/TFLite/results/report"
os.makedirs(REPORT_DIR, exist_ok=True)

TXT_REPORT_PATH = os.path.join(REPORT_DIR, f"tflite_int8_{os.path.splitext(os.path.basename(MODEL_PATH))[0]}.txt")

CSV_REPORT_DIR = "/content/drive/MyDrive"
os.makedirs(CSV_REPORT_DIR, exist_ok=True)
CSV_REPORT_PATH = os.path.join(CSV_REPORT_DIR, "summary_comparison.csv")


# === Load TFLite Model ===
interpreter = tf.lite.Interpreter(model_path=MODEL_PATH)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# === Prepare Dataset ===
class_names = sorted([d for d in os.listdir(IMG_DIR) if os.path.isdir(os.path.join(IMG_DIR, d))])
class_to_index = {cls_name: idx for idx, cls_name in enumerate(class_names)}

image_paths = []
image_labels = []
for class_name in class_names:
    class_dir = os.path.join(IMG_DIR, class_name)
    for img_file in os.listdir(class_dir):
        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):
            image_paths.append(os.path.join(class_dir, img_file))
            image_labels.append(class_name)

# === Preprocessing Function ===
def preprocess_image(img_path):
    img = Image.open(img_path).convert('RGB')
    img = img.resize((IMG_SIZE, IMG_SIZE))
    x = np.array(img, dtype=np.float32) / 255.0
    x = np.expand_dims(x, axis=0)
    return x

# === Setup CPU memory tracking ===
process = psutil.Process(os.getpid())
initial_cpu_mem = process.memory_info().rss
peak_cpu_mem = initial_cpu_mem

# === Inference ===
y_true, y_pred = [], []
total_images = 0
total_time_all = 0.0
min_time_all = float('inf')

start_time_all = time.time()
print(f"Running FP16 TFLite inference {NUM_REPEATS} times per image on {DEVICE} ({ARCHITECTURE})...\n")

for img_path, true_class in tqdm(zip(image_paths, image_labels), total=len(image_paths), desc="Processing Images"):
    try:
        x = preprocess_image(img_path)
    except Exception as e:
        print(f"Error opening {img_path}: {e}")
        continue

    times = []
    gc.collect()

    for _ in range(NUM_REPEATS):
        start_time = time.time()
        interpreter.set_tensor(input_details[0]['index'], x)
        interpreter.invoke()
        output_data = interpreter.get_tensor(output_details[0]['index'])
        end_time = time.time()
        times.append(end_time - start_time)

    avg_time = sum(times) / NUM_REPEATS
    min_time = min(times)

    predicted_index = np.argmax(output_data, axis=1)[0]
    true_index = class_to_index.get(true_class, -1)
    if true_index == -1:
        print(f"Warning: class '{true_class}' not found. Skipping.")
        continue

    y_true.append(true_index)
    y_pred.append(predicted_index)
    total_images += 1
    total_time_all += avg_time
    min_time_all = min(min_time_all, min_time)

    # CPU memory tracking
    current_cpu_mem = process.memory_info().rss
    peak_cpu_mem = max(peak_cpu_mem, current_cpu_mem)

# === Final Metrics ===
end_time_all = time.time()
total_runtime = end_time_all - start_time_all

# Since TFLite CPU-only, GPU stats = 0
peak_gpu_alloc = 0
peak_gpu_reserved = 0
total_gpu_mem_used = 0
total_combined_mem = (peak_cpu_mem - initial_cpu_mem) / (1024 ** 2) + total_gpu_mem_used

accuracy = accuracy_score(y_true, y_pred)
avg_inference_time = (total_time_all / total_images) * 1000
min_inference_time = min_time_all * 1000
report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)

# === Console Output ===
print(f"\n=== TFLite FP16 Inference Report ({ARCHITECTURE}) ===")
print(f"Model: {os.path.basename(MODEL_PATH)}")
print(f"Accuracy: {accuracy * 100:.2f}%")
print(f"Average Inference Time: {avg_inference_time:.2f} ms")
print(f"Minimum Inference Time: {min_inference_time:.2f} ms")
print(f"Peak CPU Memory: {(peak_cpu_mem - initial_cpu_mem) / (1024 ** 2):.2f} MB")
print(f"Total Memory Used (CPU+GPU): {total_combined_mem:.2f} MB")
print(f"Total Runtime: {total_runtime:.2f} seconds")
print("Per-class Performance:")
print(report)

# === Save TXT Report ===
with open(TXT_REPORT_PATH, "w") as f:
    f.write(f"=== TFLite FP16 Inference Report ({ARCHITECTURE}) ===\n")
    f.write(f"Model: {os.path.basename(MODEL_PATH)}\n")
    f.write(f"Accuracy: {accuracy * 100:.2f}%\n")
    f.write(f"Average Inference Time: {avg_inference_time:.2f} ms\n")
    f.write(f"Minimum Inference Time: {min_inference_time:.2f} ms\n")
    f.write(f"Peak CPU Memory: {(peak_cpu_mem - initial_cpu_mem) / (1024 ** 2):.2f} MB\n")
    f.write(f"Total Memory Used (CPU+GPU): {total_combined_mem:.2f} MB\n")
    f.write(f"Total Runtime: {total_runtime:.2f} seconds\n\n")
    f.write("Per-class Performance:\n")
    f.write(report)
print(f"Saved detailed inference report to {TXT_REPORT_PATH}")

# === Save/Append CSV Summary ===
summary = {
    "Timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    "Model": os.path.basename(MODEL_PATH),
    "Architecture": ARCHITECTURE,
    "Format": FORMAT_TYPE,
    "Runtime": RUNTIME,
    "Technique": "FP16 TFLite",
    "Accuracy (%)": round(accuracy * 100, 2),
    "Avg Inference Time (ms)": round(avg_inference_time, 2),
    "Min Inference Time (ms)": round(min_inference_time, 2),
    "Peak GPU Alloc (MB)": round(peak_gpu_alloc, 2),
    "Peak GPU Reserved (MB)": round(peak_gpu_reserved, 2),
    "Peak CPU Mem (MB)": round((peak_cpu_mem - initial_cpu_mem) / (1024 ** 2), 2),
    "Total GPU Mem Used (MB)": round(total_gpu_mem_used, 2),
    "Total Mem Used (MB)": round(total_combined_mem, 2),
    "Total Runtime (s)": round(total_runtime, 2),
    "device": "CPU",
    "Repetitions": NUM_REPEATS
}

if os.path.exists(CSV_REPORT_PATH):
    df = pd.read_csv(CSV_REPORT_PATH)
    df = pd.concat([df, pd.DataFrame([summary])], ignore_index=True)
else:
    df = pd.DataFrame([summary])

df.to_csv(CSV_REPORT_PATH, index=False)
print(f"Appended summary to {CSV_REPORT_PATH}")